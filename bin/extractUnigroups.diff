--- WikiExtractor.py	2014-07-21 22:25:28.000000000 +0200
+++ WikiExtractUnigroups.py	2014-07-21 22:22:29.000000000 +0200
@@ -4,14 +4,14 @@
 # =============================================================================
 #  Version: 2.6 (Oct 14, 2013)
 #  Author: Giuseppe Attardi (attardi@di.unipi.it), University of Pisa
-#	   Antonio Fuschetto (fuschett@di.unipi.it), University of Pisa
+#      Antonio Fuschetto (fuschett@di.unipi.it), University of Pisa
 #
 #  Contributors:
-#	Leonardo Souza (lsouza@amtera.com.br)
-#	Juan Manuel Caicedo (juan@cavorite.com)
-#	Humberto Pereira (begini@gmail.com)
-#	Siegfried-A. Gevatter (siegfried@gevatter.com)
-#	Pedro Assis (pedroh2306@gmail.com)
+#   Leonardo Souza (lsouza@amtera.com.br)
+#   Juan Manuel Caicedo (juan@cavorite.com)
+#   Humberto Pereira (begini@gmail.com)
+#   Siegfried-A. Gevatter (siegfried@gevatter.com)
+#   Pedro Assis (pedroh2306@gmail.com)
 #
 # =============================================================================
 #  Copyright (c) 2009. Giuseppe Attardi (attardi@di.unipi.it).
@@ -36,8 +36,8 @@
 number of files of similar size in a given directory.
 Each file contains several documents in Tanl document format:
 	<doc id="" url="" title="">
-        ...
-        </doc>
+		...
+		</doc>
 
 Usage:
   WikiExtractor.py [options]
@@ -49,8 +49,8 @@
   -l, --link            : preserve links
   -n NS, --ns NS        : accepted namespaces (separated by commas)
   -o, --output= dir     : place output files in specified directory (default
-                          current)
-  -s, --sections	: preserve sections
+						  current)
+  -s, --sections    : preserve sections
   -h, --help            : display this help and exit
 """
 
@@ -61,6 +61,7 @@
 import re
 import bz2
 import os.path
+import unicodedata
 from htmlentitydefs import name2codepoint
 
 ### PARAMS ####################################################################
@@ -90,12 +91,27 @@
 # Drop these elements from article text
 #
 discardElements = set([
-        'gallery', 'timeline', 'noinclude', 'pre',
-        'table', 'tr', 'td', 'th', 'caption',
-        'form', 'input', 'select', 'option', 'textarea',
-        'ul', 'li', 'ol', 'dl', 'dt', 'dd', 'menu', 'dir',
-        'ref', 'references', 'img', 'imagemap', 'source'
-        ])
+		'gallery', 'timeline', 'noinclude', 'pre',
+		'table', 'tr', 'td', 'th', 'caption',
+		'form', 'input', 'select', 'option', 'textarea',
+		'ul', 'li', 'ol', 'dl', 'dt', 'dd', 'menu', 'dir',
+		'ref', 'references', 'img', 'imagemap', 'source'
+		])
+
+##
+# List of words that should be concatenated while parsing
+#
+concatenatedWords = []
+
+##
+# List of unicode groups
+#
+UNIGROUPS = ()
+
+##
+# Flag whether all words should be converted to lower case
+#
+LOWCASE = False
 
 #=========================================================================
 #
@@ -114,24 +130,61 @@
 # Program version
 version = '2.5'
 
+def parseUnicodeGroups(uni_cmd_arg):
+	"""Parses a string into pairs of 2 characters that will be used to check
+	against unicode groups to filter out characters.
+	"""
+
+	if len(uni_cmd_arg) % 2 == 1:
+		print >> sys.stderr, "Unicode groups string malformed!"
+		print >> sys.stderr, "Expected: {Ll|Nd|...}"
+		print >> sys.stderr, "Got: " + uni_cmd_arg
+		return None
+
+	return tuple(re.findall("..", uni_cmd_arg))
+
+def filterCharacters(line):
+	"""Method that checks whether each character in a line is in the character
+	set described in UNIGROUPS, and removes every other character.
+	"""
+
+	result = []
+	for char in line:
+		# If a specific character--like an apostrophe--should never get
+		# removed, add these lines:
+		# (Replace "APOSTROPHE" with the name returned by unicodedata.name())
+		# if unicodedata.name(char) == "APOSTROPHE":
+			# result.append(char)
+			# continue
+		char = unicodedata.category(char) in UNIGROUPS and char or u'#'
+		result.append(char)
+	return u''.join(result).replace(u'#', u' ')
+
 ##### Main function ###########################################################
 
 def WikiDocument(out, id, title, text):
-    url = get_url(id, prefix)
-    header = '<doc id="%s" url="%s" title="%s">\n' % (id, url, title)
-    # Separate header from text with a newline.
-    header += title + '\n'
-    header = header.encode('utf-8')
-    text = clean(text)
-    footer = "\n</doc>"
-    out.reserve(len(header) + len(text) + len(footer))
-    print >> out, header
-    for line in compact(text):
-        print >> out, line.encode('utf-8')
-    print >> out, footer
+	url = get_url(id, prefix)
+	text = clean(text)
+	out.reserve(len(text))
+	
+	for line in compact(text):
+		if LOWCASE == True:
+			line = line.lower()
+
+		if UNIGROUPS is not ():
+			line = filterCharacters(line)
+			
+		if any(word in line for word in concatenatedWords):
+			for word in concatenatedWords:
+				line = line.replace(word, word.replace(" ", "_"))
+		
+		line = re.sub(" +", " ", line)
+
+
+		print >> out, line.encode('utf-8')
 
 def get_url(id, prefix):
-    return "%s?curid=%s" % (prefix, id)
+	return "%s?curid=%s" % (prefix, id)
 
 #------------------------------------------------------------------------------
 
@@ -139,10 +192,10 @@
 
 # handle 'a' separetely, depending on keepLinks
 ignoredTags = [
-        'b', 'big', 'blockquote', 'center', 'cite', 'div', 'em',
-        'font', 'h1', 'h2', 'h3', 'h4', 'hiero', 'i', 'kbd', 'nowiki',
-        'p', 'plaintext', 's', 'small', 'span', 'strike', 'strong',
-        'sub', 'sup', 'tt', 'u', 'var',
+		'b', 'big', 'blockquote', 'center', 'cite', 'div', 'em',
+		'font', 'h1', 'h2', 'h3', 'h4', 'hiero', 'i', 'kbd', 'nowiki',
+		'p', 'plaintext', 's', 'small', 'span', 'strike', 'strong',
+		'sub', 'sup', 'tt', 'u', 'var',
 ]
 
 placeholder_tags = {'math':'formula', 'code':'codice'}
@@ -157,32 +210,32 @@
 
   m = re.compile(r'([^:]*):(\s*)(\S(?:.*))').match(title)
   if m:
-      prefix = m.group(1)
-      if m.group(2):
-          optionalWhitespace = ' '
-      else:
-          optionalWhitespace = ''
-      rest = m.group(3)
-
-      ns = prefix.capitalize()
-      if ns in acceptedNamespaces:
-          # If the prefix designates a known namespace, then it might be
-          # followed by optional whitespace that should be removed to get
-          # the canonical page name
-          # (e.g., "Category:  Births" should become "Category:Births").
-          title = ns + ":" + rest.capitalize()
-      else:
-          # No namespace, just capitalize first letter.
+	  prefix = m.group(1)
+	  if m.group(2):
+		  optionalWhitespace = ' '
+	  else:
+		  optionalWhitespace = ''
+	  rest = m.group(3)
+
+	  ns = prefix.capitalize()
+	  if ns in acceptedNamespaces:
+		  # If the prefix designates a known namespace, then it might be
+		  # followed by optional whitespace that should be removed to get
+		  # the canonical page name
+		  # (e.g., "Category:  Births" should become "Category:Births").
+		  title = ns + ":" + rest.capitalize()
+	  else:
+		  # No namespace, just capitalize first letter.
 	  # If the part before the colon is not a known namespace, then we must
-          # not remove the space after the colon (if any), e.g.,
-          # "3001: The_Final_Odyssey" != "3001:The_Final_Odyssey".
-          # However, to get the canonical page name we must contract multiple
-          # spaces into one, because
-          # "3001:   The_Final_Odyssey" != "3001: The_Final_Odyssey".
-          title = prefix.capitalize() + ":" + optionalWhitespace + rest
+		  # not remove the space after the colon (if any), e.g.,
+		  # "3001: The_Final_Odyssey" != "3001:The_Final_Odyssey".
+		  # However, to get the canonical page name we must contract multiple
+		  # spaces into one, because
+		  # "3001:   The_Final_Odyssey" != "3001: The_Final_Odyssey".
+		  title = prefix.capitalize() + ":" + optionalWhitespace + rest
   else:
-      # no namespace, just capitalize first letter
-      title = title.capitalize();
+	  # no namespace, just capitalize first letter
+	  title = title.capitalize();
   return title
 
 ##
@@ -192,21 +245,21 @@
 # @return The plain text, as a Unicode string, if necessary.
 
 def unescape(text):
-    def fixup(m):
-        text = m.group(0)
-        code = m.group(1)
-        try:
-            if text[1] == "#":  # character reference
-                if text[2] == "x":
-                    return unichr(int(code[1:], 16))
-                else:
-                    return unichr(int(code))
-            else:               # named entity
-                return unichr(name2codepoint[code])
-        except:
-            return text # leave as is
+	def fixup(m):
+		text = m.group(0)
+		code = m.group(1)
+		try:
+			if text[1] == "#":  # character reference
+				if text[2] == "x":
+					return unichr(int(code[1:], 16))
+				else:
+					return unichr(int(code))
+			else:               # named entity
+				return unichr(name2codepoint[code])
+		except:
+			return text # leave as is
 
-    return re.sub("&#?(\w+);", fixup, text)
+	return re.sub("&#?(\w+);", fixup, text)
 
 # Match HTML comments
 comment = re.compile(r'<!--.*?-->', re.DOTALL)
@@ -214,30 +267,30 @@
 # Match elements to ignore
 discard_element_patterns = []
 for tag in discardElements:
-    pattern = re.compile(r'<\s*%s\b[^>]*>.*?<\s*/\s*%s>' % (tag, tag), re.DOTALL | re.IGNORECASE)
-    discard_element_patterns.append(pattern)
+	pattern = re.compile(r'<\s*%s\b[^>]*>.*?<\s*/\s*%s>' % (tag, tag), re.DOTALL | re.IGNORECASE)
+	discard_element_patterns.append(pattern)
 
 # Match ignored tags
 ignored_tag_patterns = []
 def ignoreTag(tag):
-    left = re.compile(r'<\s*%s\b[^>]*>' % tag, re.IGNORECASE)
-    right = re.compile(r'<\s*/\s*%s>' % tag, re.IGNORECASE)
-    ignored_tag_patterns.append((left, right))
+	left = re.compile(r'<\s*%s\b[^>]*>' % tag, re.IGNORECASE)
+	right = re.compile(r'<\s*/\s*%s>' % tag, re.IGNORECASE)
+	ignored_tag_patterns.append((left, right))
 
 for tag in ignoredTags:
-    ignoreTag(tag)
+	ignoreTag(tag)
 
 # Match selfClosing HTML tags
 selfClosing_tag_patterns = []
 for tag in selfClosingTags:
-    pattern = re.compile(r'<\s*%s\b[^/]*/\s*>' % tag, re.DOTALL | re.IGNORECASE)
-    selfClosing_tag_patterns.append(pattern)
+	pattern = re.compile(r'<\s*%s\b[^/]*/\s*>' % tag, re.DOTALL | re.IGNORECASE)
+	selfClosing_tag_patterns.append(pattern)
 
 # Match HTML placeholder tags
 placeholder_tag_patterns = []
 for tag, repl in placeholder_tags.items():
-    pattern = re.compile(r'<\s*%s(\s*| [^>]+?)>.*?<\s*/\s*%s\s*>' % (tag, tag), re.DOTALL | re.IGNORECASE)
-    placeholder_tag_patterns.append((pattern, repl))
+	pattern = re.compile(r'<\s*%s(\s*| [^>]+?)>.*?<\s*/\s*%s\s*>' % (tag, tag), re.DOTALL | re.IGNORECASE)
+	placeholder_tag_patterns.append((pattern, repl))
 
 # Match preformatted lines
 preformatted = re.compile(r'^ .*?$', re.MULTILINE)
@@ -261,70 +314,70 @@
 
 # A matching function for nested expressions, e.g. namespaces and tables.
 def dropNested(text, openDelim, closeDelim):
-    openRE = re.compile(openDelim)
-    closeRE = re.compile(closeDelim)
-    # partition text in separate blocks { } { }
-    matches = []                # pairs (s, e) for each partition
-    nest = 0                    # nesting level
-    start = openRE.search(text, 0)
-    if not start:
-        return text
-    end = closeRE.search(text, start.end())
-    next = start
-    while end:
-        next = openRE.search(text, next.end())
-        if not next:            # termination
-            while nest:         # close all pending
-                nest -=1
-                end0 = closeRE.search(text, end.end())
-                if end0:
-                    end = end0
-                else:
-                    break
-            matches.append((start.start(), end.end()))
-            break
-        while end.end() < next.start():
-            # { } {
-            if nest:
-                nest -= 1
-                # try closing more
-                last = end.end()
-                end = closeRE.search(text, end.end())
-                if not end:     # unbalanced
-                    if matches:
-                        span = (matches[0][0], last)
-                    else:
-                        span = (start.start(), last)
-                    matches = [span]
-                    break
-            else:
-                matches.append((start.start(), end.end()))
-                # advance start, find next close
-                start = next
-                end = closeRE.search(text, next.end())
-                break           # { }
-        if next != start:
-            # { { }
-            nest += 1
-    # collect text outside partitions
-    res = ''
-    start = 0
-    for s, e in  matches:
-        res += text[start:s]
-        start = e
-    res += text[start:]
-    return res
+	openRE = re.compile(openDelim)
+	closeRE = re.compile(closeDelim)
+	# partition text in separate blocks { } { }
+	matches = []                # pairs (s, e) for each partition
+	nest = 0                    # nesting level
+	start = openRE.search(text, 0)
+	if not start:
+		return text
+	end = closeRE.search(text, start.end())
+	next = start
+	while end:
+		next = openRE.search(text, next.end())
+		if not next:            # termination
+			while nest:         # close all pending
+				nest -=1
+				end0 = closeRE.search(text, end.end())
+				if end0:
+					end = end0
+				else:
+					break
+			matches.append((start.start(), end.end()))
+			break
+		while end.end() < next.start():
+			# { } {
+			if nest:
+				nest -= 1
+				# try closing more
+				last = end.end()
+				end = closeRE.search(text, end.end())
+				if not end:     # unbalanced
+					if matches:
+						span = (matches[0][0], last)
+					else:
+						span = (start.start(), last)
+					matches = [span]
+					break
+			else:
+				matches.append((start.start(), end.end()))
+				# advance start, find next close
+				start = next
+				end = closeRE.search(text, next.end())
+				break           # { }
+		if next != start:
+			# { { }
+			nest += 1
+	# collect text outside partitions
+	res = ''
+	start = 0
+	for s, e in  matches:
+		res += text[start:s]
+		start = e
+	res += text[start:]
+	return res
 
 def dropSpans(matches, text):
-    """Drop from text the blocks identified in matches"""
-    matches.sort()
-    res = ''
-    start = 0
-    for s, e in  matches:
-        res += text[start:s]
-        start = e
-    res += text[start:]
-    return res
+	"""Drop from text the blocks identified in matches"""
+	matches.sort()
+	res = ''
+	start = 0
+	for s, e in  matches:
+		res += text[start:s]
+		start = e
+	res += text[start:]
+	return res
 
 # Match interwiki links, | separates parameters.
 # First parameter is displayed, also trailing concatenated text included
@@ -339,349 +392,364 @@
 
 # Function applied to wikiLinks
 def make_anchor_tag(match):
-    global keepLinks
-    link = match.group(1)
-    colon = link.find(':')
-    if colon > 0 and link[:colon] not in acceptedNamespaces:
-        return ''
-    trail = match.group(3)
-    anchor = match.group(2)
-    if not anchor:
-        anchor = link
-    anchor += trail
-    if keepLinks:
-        return '<a href="%s">%s</a>' % (link, anchor)
-    else:
-        return anchor
+	global keepLinks
+	link = match.group(1)
+	colon = link.find(':')
+	if colon > 0 and link[:colon] not in acceptedNamespaces:
+		return ''
+	trail = match.group(3)
+	anchor = match.group(2)
+	if not anchor:
+		anchor = link
+	anchor += trail
+	if keepLinks:
+		return '<a href="%s">%s</a>' % (link, anchor)
+	else:
+		return anchor
 
 def clean(text):
 
-    # FIXME: templates should be expanded
-    # Drop transclusions (template, parser functions)
-    # See: http://www.mediawiki.org/wiki/Help:Templates
-    text = dropNested(text, r'{{', r'}}')
-
-    # Drop tables
-    text = dropNested(text, r'{\|', r'\|}')
-
-    # Expand links
-    text = wikiLink.sub(make_anchor_tag, text)
-    # Drop all remaining ones
-    text = parametrizedLink.sub('', text)
-
-    # Handle external links
-    text = externalLink.sub(r'\1', text)
-    text = externalLinkNoAnchor.sub('', text)
-
-    # Handle bold/italic/quote
-    text = bold_italic.sub(r'\1', text)
-    text = bold.sub(r'\1', text)
-    text = italic_quote.sub(r'&quot;\1&quot;', text)
-    text = italic.sub(r'&quot;\1&quot;', text)
-    text = quote_quote.sub(r'\1', text)
-    text = text.replace("'''", '').replace("''", '&quot;')
-
-    ################ Process HTML ###############
-
-    # turn into HTML
-    text = unescape(text)
-    # do it again (&amp;nbsp;)
-    text = unescape(text)
-
-    # Collect spans
-
-    matches = []
-    # Drop HTML comments
-    for m in comment.finditer(text):
-            matches.append((m.start(), m.end()))
-
-    # Drop self-closing tags
-    for pattern in selfClosing_tag_patterns:
-        for m in pattern.finditer(text):
-            matches.append((m.start(), m.end()))
-
-    # Drop ignored tags
-    for left, right in ignored_tag_patterns:
-        for m in left.finditer(text):
-            matches.append((m.start(), m.end()))
-        for m in right.finditer(text):
-            matches.append((m.start(), m.end()))
-
-    # Bulk remove all spans
-    text = dropSpans(matches, text)
-
-    # Cannot use dropSpan on these since they may be nested
-    # Drop discarded elements
-    for pattern in discard_element_patterns:
-        text = pattern.sub('', text)
-
-    # Expand placeholders
-    for pattern, placeholder in placeholder_tag_patterns:
-        index = 1
-        for match in pattern.finditer(text):
-            text = text.replace(match.group(), '%s_%d' % (placeholder, index))
-            index += 1
-
-    text = text.replace('<<', u'«').replace('>>', u'»')
-
-    #############################################
-
-    # Drop preformatted
-    # This can't be done before since it may remove tags
-    text = preformatted.sub('', text)
-
-    # Cleanup text
-    text = text.replace('\t', ' ')
-    text = spaces.sub(' ', text)
-    text = dots.sub('...', text)
-    text = re.sub(u' (,:\.\)\]»)', r'\1', text)
-    text = re.sub(u'(\[\(«) ', r'\1', text)
-    text = re.sub(r'\n\W+?\n', '\n', text) # lines with only punctuations
-    text = text.replace(',,', ',').replace(',.', '.')
-    return text
+	# FIXME: templates should be expanded
+	# Drop transclusions (template, parser functions)
+	# See: http://www.mediawiki.org/wiki/Help:Templates
+	text = dropNested(text, r'{{', r'}}')
+
+	# Drop tables
+	text = dropNested(text, r'{\|', r'\|}')
+
+	# Expand links
+	text = wikiLink.sub(make_anchor_tag, text)
+	# Drop all remaining ones
+	text = parametrizedLink.sub('', text)
+
+	# Handle external links
+	text = externalLink.sub(r'\1', text)
+	text = externalLinkNoAnchor.sub('', text)
+
+	# Handle bold/italic/quote
+	text = bold_italic.sub(r'\1', text)
+	text = bold.sub(r'\1', text)
+	text = italic_quote.sub(r'&quot;\1&quot;', text)
+	text = italic.sub(r'&quot;\1&quot;', text)
+	text = quote_quote.sub(r'\1', text)
+	text = text.replace("'''", '').replace("''", '&quot;')
+
+	################ Process HTML ###############
+
+	# turn into HTML
+	text = unescape(text)
+	# do it again (&amp;nbsp;)
+	text = unescape(text)
+
+	# Collect spans
+
+	matches = []
+	# Drop HTML comments
+	for m in comment.finditer(text):
+			matches.append((m.start(), m.end()))
+
+	# Drop self-closing tags
+	for pattern in selfClosing_tag_patterns:
+		for m in pattern.finditer(text):
+			matches.append((m.start(), m.end()))
+
+	# Drop ignored tags
+	for left, right in ignored_tag_patterns:
+		for m in left.finditer(text):
+			matches.append((m.start(), m.end()))
+		for m in right.finditer(text):
+			matches.append((m.start(), m.end()))
+
+	# Bulk remove all spans
+	text = dropSpans(matches, text)
+
+	# Cannot use dropSpan on these since they may be nested
+	# Drop discarded elements
+	for pattern in discard_element_patterns:
+		text = pattern.sub('', text)
+
+	# Expand placeholders
+	for pattern, placeholder in placeholder_tag_patterns:
+		index = 1
+		for match in pattern.finditer(text):
+			text = text.replace(match.group(), '%s_%d' % (placeholder, index))
+			index += 1
+
+	text = text.replace('<<', u'«').replace('>>', u'»')
+
+	#############################################
+
+	# Drop preformatted
+	# This can't be done before since it may remove tags
+	text = preformatted.sub('', text)
+
+	# Cleanup text
+	text = text.replace('\t', ' ')
+	text = spaces.sub(' ', text)
+	text = dots.sub('...', text)
+	text = re.sub(u' (,:\.\)\]»)', r'\1', text)
+	text = re.sub(u'(\[\(«) ', r'\1', text)
+	text = re.sub(r'\n\W+?\n', '\n', text) # lines with only punctuations
+	text = text.replace(',,', ',').replace(',.', '.')
+	return text
 
 section = re.compile(r'(==+)\s*(.*?)\s*\1')
 
 def compact(text):
-    """Deal with headers, lists, empty sections, residuals of tables"""
-    page = []                   # list of paragraph
-    headers = {}                # Headers for unfilled sections
-    emptySection = False        # empty sections are discarded
-    inList = False              # whether opened <UL>
-
-    for line in text.split('\n'):
-
-        if not line:
-            continue
-        # Handle section titles
-        m = section.match(line)
-        if m:
-            title = m.group(2)
-            lev = len(m.group(1))
-            if keepSections:
-                page.append("<h%d>%s</h%d>" % (lev, title, lev))
-            if title and title[-1] not in '!?':
-                title += '.'
-            headers[lev] = title
-            # drop previous headers
-            for i in headers.keys():
-                if i > lev:
-                    del headers[i]
-            emptySection = True
-            continue
-        # Handle page title
-        if line.startswith('++'):
-            title = line[2:-2]
-            if title:
-                if title[-1] not in '!?':
-                    title += '.'
-                page.append(title)
-        # handle lists
-        elif line[0] in '*#:;':
-            if keepSections:
-                page.append("<li>%s</li>" % line[1:])
-            else:
-                continue
-        # Drop residuals of lists
-        elif line[0] in '{|' or line[-1] in '}':
-            continue
-        # Drop irrelevant lines
-        elif (line[0] == '(' and line[-1] == ')') or line.strip('.-') == '':
-            continue
-        elif len(headers):
-            items = headers.items()
-            items.sort()
-            for (i, v) in items:
-                page.append(v)
-            headers.clear()
-            page.append(line)   # first line
-            emptySection = False
-        elif not emptySection:
-            page.append(line)
+	"""Deal with headers, lists, empty sections, residuals of tables"""
+	page = []                   # list of paragraph
+	headers = {}                # Headers for unfilled sections
+	emptySection = False        # empty sections are discarded
+	inList = False              # whether opened <UL>
+
+	for line in text.split('\n'):
+
+		if not line:
+			continue
+		# Handle section titles
+		m = section.match(line)
+		if m:
+			title = m.group(2)
+			lev = len(m.group(1))
+			if keepSections:
+				page.append("<h%d>%s</h%d>" % (lev, title, lev))
+			if title and title[-1] not in '!?':
+				title += '.'
+			headers[lev] = title
+			# drop previous headers
+			for i in headers.keys():
+				if i > lev:
+					del headers[i]
+			emptySection = True
+			continue
+		# Handle page title
+		if line.startswith('++'):
+			title = line[2:-2]
+			if title:
+				if title[-1] not in '!?':
+					title += '.'
+				page.append(title)
+		# handle lists
+		elif line[0] in '*#:;':
+			if keepSections:
+				page.append("<li>%s</li>" % line[1:])
+			else:
+				continue
+		# Drop residuals of lists
+		elif line[0] in '{|' or line[-1] in '}':
+			continue
+		# Drop irrelevant lines
+		elif (line[0] == '(' and line[-1] == ')') or line.strip('.-') == '':
+			continue
+		elif len(headers):
+			items = headers.items()
+			items.sort()
+			for (i, v) in items:
+				page.append(v)
+			headers.clear()
+			page.append(line)   # first line
+			emptySection = False
+		elif not emptySection:
+			page.append(line)
 
-    return page
+	return page
 
 def handle_unicode(entity):
-    numeric_code = int(entity[2:-1])
-    if numeric_code >= 0x10000: return ''
-    return unichr(numeric_code)
+	numeric_code = int(entity[2:-1])
+	if numeric_code >= 0x10000: return ''
+	return unichr(numeric_code)
 
 #------------------------------------------------------------------------------
 
 class OutputSplitter:
-    def __init__(self, compress, max_file_size, path_name):
-        self.dir_index = 0
-        self.file_index = -1
-        self.compress = compress
-        self.max_file_size = max_file_size
-        self.path_name = path_name
-        self.out_file = self.open_next_file()
-
-    def reserve(self, size):
-        cur_file_size = self.out_file.tell()
-        if cur_file_size + size > self.max_file_size:
-            self.close()
-            self.out_file = self.open_next_file()
-
-    def write(self, text):
-        self.out_file.write(text)
-
-    def close(self):
-        self.out_file.close()
-
-    def open_next_file(self):
-        self.file_index += 1
-        if self.file_index == 100:
-            self.dir_index += 1
-            self.file_index = 0
-        dir_name = self.dir_name()
-        if not os.path.isdir(dir_name):
-            os.makedirs(dir_name)
-        file_name = os.path.join(dir_name, self.file_name())
-        if self.compress:
-            return bz2.BZ2File(file_name + '.bz2', 'w')
-        else:
-            return open(file_name, 'w')
-
-    def dir_name(self):
-        char1 = self.dir_index % 26
-        char2 = self.dir_index / 26 % 26
-        return os.path.join(self.path_name, '%c%c' % (ord('A') + char2, ord('A') + char1))
+	def __init__(self, compress, max_file_size, path_name):
+		self.dir_index = 0
+		self.file_index = -1
+		self.compress = compress
+		self.max_file_size = max_file_size
+		self.path_name = path_name
+		self.out_file = self.open_next_file()
+
+	def reserve(self, size):
+		cur_file_size = self.out_file.tell()
+		if cur_file_size + size > self.max_file_size:
+			self.close()
+			self.out_file = self.open_next_file()
+
+	def write(self, text):
+		self.out_file.write(text)
+
+	def close(self):
+		self.out_file.close()
+
+	def open_next_file(self):
+		self.file_index += 1
+		if self.file_index == 100:
+			self.dir_index += 1
+			self.file_index = 0
+		dir_name = self.dir_name()
+		if not os.path.isdir(dir_name):
+			os.makedirs(dir_name)
+		file_name = os.path.join(dir_name, self.file_name())
+		if self.compress:
+			return bz2.BZ2File(file_name + '.bz2', 'w')
+		else:
+			return open(file_name, 'w')
+
+	def dir_name(self):
+		char1 = self.dir_index % 26
+		char2 = self.dir_index / 26 % 26
+		return os.path.join(self.path_name, '%c%c' % (ord('A') + char2, ord('A') + char1))
 
-    def file_name(self):
-        return 'wiki_%02d' % self.file_index
+	def file_name(self):
+		return 'wiki_%02d' % self.file_index
 
 ### READER ###################################################################
 
 tagRE = re.compile(r'(.*?)<(/?\w+)[^>]*>(?:([^<]*)(<.*?>)?)?')
 
 def process_data(input, output):
-    global prefix
+	global prefix
 
-    page = []
-    id = None
-    inText = False
-    redirect = False
-    for line in input:
-        line = line.decode('utf-8')
-        tag = ''
-        if '<' in line:
-            m = tagRE.search(line)
-            if m:
-                tag = m.group(2)
-        if tag == 'page':
-            page = []
-            redirect = False
-        elif tag == 'id' and not id:
-            id = m.group(3)
-        elif tag == 'title':
-            title = m.group(3)
-        elif tag == 'redirect':
-            redirect = True
-        elif tag == 'text':
-            inText = True
-            line = line[m.start(3):m.end(3)] + '\n'
-            page.append(line)
-            if m.lastindex == 4: # open-close
-                inText = False
-        elif tag == '/text':
-            if m.group(1):
-                page.append(m.group(1) + '\n')
-            inText = False
-        elif inText:
-            page.append(line)
-        elif tag == '/page':
-            colon = title.find(':')
-            if (colon < 0 or title[:colon] in acceptedNamespaces) and \
-                    not redirect:
-                print id, title.encode('utf-8')
-                sys.stdout.flush()
-                WikiDocument(output, id, title, ''.join(page))
-            id = None
-            page = []
-        elif tag == 'base':
-            # discover prefix from the xml dump file
-            # /mediawiki/siteinfo/base
-            base = m.group(3)
-            prefix = base[:base.rfind("/")]
+	page = []
+	id = None
+	inText = False
+	redirect = False
+	for line in input:
+		line = line.decode('utf-8')
+		tag = ''
+		if '<' in line:
+			m = tagRE.search(line)
+			if m:
+				tag = m.group(2)
+		if tag == 'page':
+			page = []
+			redirect = False
+		elif tag == 'id' and not id:
+			id = m.group(3)
+		elif tag == 'title':
+			title = m.group(3)
+		elif tag == 'redirect':
+			redirect = True
+		elif tag == 'text':
+			inText = True
+			line = line[m.start(3):m.end(3)] + '\n'
+			page.append(line)
+			if m.lastindex == 4: # open-close
+				inText = False
+		elif tag == '/text':
+			if m.group(1):
+				page.append(m.group(1) + '\n')
+			inText = False
+		elif inText:
+			page.append(line)
+		elif tag == '/page':
+			colon = title.find(':')
+			if (colon < 0 or title[:colon] in acceptedNamespaces) and \
+					not redirect:
+				print id, title.encode('utf-8')
+				sys.stdout.flush()
+				WikiDocument(output, id, title, ''.join(page))
+			id = None
+			page = []
+		elif tag == 'base':
+			# discover prefix from the xml dump file
+			# /mediawiki/siteinfo/base
+			base = m.group(3)
+			prefix = base[:base.rfind("/")]
 
 ### CL INTERFACE ############################################################
 
 def show_help():
-    print >> sys.stdout, __doc__,
+	print >> sys.stdout, __doc__,
 
 def show_usage(script_name):
-    print >> sys.stderr, 'Usage: %s [options]' % script_name
+	print >> sys.stderr, 'Usage: %s [options]' % script_name
 
 ##
 # Minimum size of output files
 minFileSize = 200 * 1024
 
 def main():
-    global keepLinks, keepSections, prefix, acceptedNamespaces
-    script_name = os.path.basename(sys.argv[0])
+	global keepLinks, keepSections, prefix, acceptedNamespaces, concatenatedWords, UNIGROUPS, LOWCASE
+	script_name = os.path.basename(sys.argv[0])
 
-    try:
-        long_opts = ['help', 'compress', 'bytes=', 'basename=', 'links', 'ns=', 'sections', 'output=', 'version']
-        opts, args = getopt.gnu_getopt(sys.argv[1:], 'cb:hln:o:B:sv', long_opts)
-    except getopt.GetoptError:
-        show_usage(script_name)
-        sys.exit(1)
-
-    compress = False
-    file_size = 500 * 1024
-    output_dir = '.'
-
-    for opt, arg in opts:
-        if opt in ('-h', '--help'):
-            show_help()
-            sys.exit()
-        elif opt in ('-c', '--compress'):
-            compress = True
-        elif opt in ('-l', '--links'):
-            keepLinks = True
-        elif opt in ('-s', '--sections'):
-            keepSections = True
-        elif opt in ('-B', '--base'):
-            prefix = arg
-        elif opt in ('-b', '--bytes'):
-            try:
-                if arg[-1] in 'kK':
-                    file_size = int(arg[:-1]) * 1024
-                elif arg[-1] in 'mM':
-                    file_size = int(arg[:-1]) * 1024 * 1024
-                else:
-                    file_size = int(arg)
-                if file_size < minFileSize: raise ValueError()
-            except ValueError:
-                print >> sys.stderr, \
-                '%s: %s: Insufficient or invalid size' % (script_name, arg)
-                sys.exit(2)
-        elif opt in ('-n', '--ns'):
-                acceptedNamespaces = set(arg.split(','))
-        elif opt in ('-o', '--output'):
-                output_dir = arg
-        elif opt in ('-v', '--version'):
-                print 'WikiExtractor.py version:', version
-                sys.exit(0)
-
-    if len(args) > 0:
-        show_usage(script_name)
-        sys.exit(4)
-
-    if not os.path.isdir(output_dir):
-        try:
-            os.makedirs(output_dir)
-        except:
-            print >> sys.stderr, 'Could not create: ', output_dir
-            return
-
-    if not keepLinks:
-        ignoreTag('a')
-
-    output_splitter = OutputSplitter(compress, file_size, output_dir)
-    process_data(sys.stdin, output_splitter)
-    output_splitter.close()
+	try:
+		long_opts = ['help', 'compress', 'bytes=', 'basename=', 'links', 'ns=', 'sections', 'output=', 'concatenate=', 'unigroups=', 'lowcase=', 'version']
+		opts, args = getopt.gnu_getopt(sys.argv[1:], 'cb:hln:o:B:sv', long_opts)
+	except getopt.GetoptError:
+		show_usage(script_name)
+		sys.exit(1)
+
+	compress = False
+	file_size = 500 * 1024
+	output_dir = '.'
+
+	for opt, arg in opts:
+		if opt in ('-h', '--help'):
+			show_help()
+			sys.exit()
+		elif opt in ('-c', '--compress'):
+			compress = True
+		elif opt in ('-l', '--links'):
+			keepLinks = True
+		elif opt in ('-s', '--sections'):
+			keepSections = True
+		elif opt in ('-B', '--base'):
+			prefix = arg
+		elif opt in ('-b', '--bytes'):
+			try:
+				if arg[-1] in 'kK':
+					file_size = int(arg[:-1]) * 1024
+				elif arg[-1] in 'mM':
+					file_size = int(arg[:-1]) * 1024 * 1024
+				else:
+					file_size = int(arg)
+				if file_size < minFileSize: raise ValueError()
+			except ValueError:
+				print >> sys.stderr, \
+				'%s: %s: Insufficient or invalid size' % (script_name, arg)
+				sys.exit(2)
+		elif opt in ('-n', '--ns'):
+				acceptedNamespaces = set(arg.split(','))
+		elif opt in ('-o', '--output'):
+				output_dir = arg
+		elif opt in ('-v', '--version'):
+				print 'WikiExtractor.py version:', version
+				sys.exit(0)
+
+		elif opt in ('--concatenate'):
+			# if unicode errors show up, replace the following line with:
+			# concatenatedWords = [f.strip().decode('utf-8') for f in open(arg, "r")]
+			concatenatedWords = [f.strip() for f in open(arg, "r")]
+			print "The following words will be concatenated if found:"
+			print concatenatedWords
+		elif opt in ('--unigroups'):
+			UNIGROUPS = parseUnicodeGroups(arg)
+			print "The following unicode groups are taken into account while parsing:"
+			print UNIGROUPS
+		elif opt in ('--lowcase'):
+			if arg:
+				LOWCASE = True
+				print "Everything will be converted to lower case."
+
+	if len(args) > 0:
+		show_usage(script_name)
+		sys.exit(4)
+
+	if not os.path.isdir(output_dir):
+		try:
+			os.makedirs(output_dir)
+		except:
+			print >> sys.stderr, 'Could not create: ', output_dir
+			return
+
+	if not keepLinks:
+		ignoreTag('a')
+
+	output_splitter = OutputSplitter(compress, file_size, output_dir)
+	process_data(sys.stdin, output_splitter)
+	output_splitter.close()
 
 if __name__ == '__main__':
-    main()
+	main()
